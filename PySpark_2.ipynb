{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epfrvqOy0XbK"
   },
   "source": [
    "# Install Java and Spark on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pf3zOXQkRf0l",
    "outputId": "6bfd4cd0-d6f3-4463-d3ad-bfb830679c74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-3.3.2-bin-hadoop3.tgz'\n"
     ]
    }
   ],
   "source": [
    "# install java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# install spark (change the version number if needed)\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "# unzip the spark file to the current folder\n",
    "!tar xf spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Llc1FhGNQ3U5"
   },
   "outputs": [],
   "source": [
    "# set your spark folder to your system path environment. \n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ExFt_N8-z2m",
    "outputId": "07a0f267-16b4-4230-c36a-c0a33fba522f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.3.2-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(spark_python, \u001b[39m\"\u001b[39;49m\u001b[39mlib\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpy4j-*.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ADMIN\\Processing\\FinalTermCoding_BigData\\FinalTerm_BigData_2.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Processing/FinalTermCoding_BigData/FinalTerm_BigData_2.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install findspark\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Processing/FinalTermCoding_BigData/FinalTerm_BigData_2.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ADMIN/Processing/FinalTermCoding_BigData/FinalTerm_BigData_2.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m findspark\u001b[39m.\u001b[39;49minit()\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m         py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(spark_python, \u001b[39m\"\u001b[39m\u001b[39mlib\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpy4j-*.zip\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to find py4j in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    163\u001b[0m                 spark_python\n\u001b[0;32m    164\u001b[0m             )\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     sys\u001b[39m.\u001b[39mpath[:\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m sys_path \u001b[39m=\u001b[39m [spark_python, py4j]\n\u001b[0;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[39m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.3.2-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "# pip install findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfAmwnpt1G5p"
   },
   "source": [
    "# Create a SparkSession in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZU-oJLNVQl45"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    "          .appName(\"Introduction to Spark\")\\\n",
    "          .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B0xdV4gLh8b"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import col, column, expr\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPpD2OQ11RV8"
   },
   "source": [
    "# Exercise 1: Data query with Spark DataFrame\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdO2c34d1WWh"
   },
   "source": [
    "##0. Load the data file: movies.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scvJmCk73r8q",
    "outputId": "c0464aca-856d-433e-a0cd-3eda2038593a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CSC14118'...\n",
      "remote: Enumerating objects: 13, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
      "remote: Total 13 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (13/13), 816.70 KiB | 1.82 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nnthaofit/CSC14118.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kNSEFq9TTmP"
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"CSC14118/movies.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGo9SwIs1xJN",
    "outputId": "46ad8961-6dba-4b01-f6a2-9e41b7cc46c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-------------------------------------------+----+\n",
      "|cast         |genres              |title                                      |year|\n",
      "+-------------+--------------------+-------------------------------------------+----+\n",
      "|[]           |[]                  |After Dark in Central Park                 |1900|\n",
      "|[]           |[]                  |Boarding School Girls' Pajama Parade       |1900|\n",
      "|[]           |[]                  |Buffalo Bill's Wild West Parad             |1900|\n",
      "|[]           |[]                  |Caught                                     |1900|\n",
      "|[]           |[]                  |Clowns Spinning Hats                       |1900|\n",
      "|[]           |[Short, Documentary]|Capture of Boer Battery by British         |1900|\n",
      "|[]           |[]                  |The Enchanted Drawing                      |1900|\n",
      "|[Paul Boyton]|[]                  |Feeding Sea Lions                          |1900|\n",
      "|[]           |[Comedy]            |How to Make a Fat Wife Out of Two Lean Ones|1900|\n",
      "|[]           |[]                  |New Life Rescue                            |1900|\n",
      "+-------------+--------------------+-------------------------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VlqG5ph3_18"
   },
   "source": [
    "## 1a. Show the schema of DataFrame that stores the movies dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmjXJMLV3-W7",
    "outputId": "a08d022d-ba06-4685-9833-e2f4e8edbddf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('cast', ArrayType(StringType(), True), True), StructField('genres', ArrayType(StringType(), True), True), StructField('title', StringType(), True), StructField('year', LongType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIVBncKQ10GL",
    "outputId": "05890424-cd87-4703-ea15-88dcf0c73616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cast: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Petw2zs7yKrt"
   },
   "source": [
    "## 1b. Show the number of distinct films in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcmqNqzeohGz",
    "outputId": "df3f59da-b76d-44fd-b43c-8f0c23a2dad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+\n",
      "|count(DISTINCT named_struct(cast, cast, genres, genres, title, title, year, year))|\n",
      "+----------------------------------------------------------------------------------+\n",
      "|                                                                             28789|\n",
      "+----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"count(distinct( cast, genres, title, year))\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIskYF2geLRE",
    "outputId": "4d9e1208-e5f7-42f6-8956-d60ba8b561eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28789"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"count(distinct( cast, genres, title, year))\").first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_otnnauXHbQA",
    "outputId": "10b71cf9-cb1a-4c2b-ca01-3aa44ed89a45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28789"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRKbh8RdyYZN"
   },
   "source": [
    "## 2. Count the number of movies released during the years 2012 and 2015 (included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Etasw6h-yguU",
    "outputId": "9b2b49d2-c086-471a-ef58-3ab5aa7beaed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(\"year >=2012 AND year <=2015\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8egu2pgyg-y"
   },
   "source": [
    "## 3. Show the year in which the number of movies released is highest. One highest year is enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXbf9__4yku-"
   },
   "outputs": [],
   "source": [
    "df1=df.groupBy(\"year\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FZl6QQqglos",
    "outputId": "e7478298-fd71-4658-97bf-a50dafe78fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|1950|  443|\n",
      "|1919|  634|\n",
      "|1936|  504|\n",
      "|1951|  429|\n",
      "|1958|  281|\n",
      "|1921|  143|\n",
      "|1983|  140|\n",
      "|1905|   35|\n",
      "|1972|  140|\n",
      "|1979|  139|\n",
      "|2007|  304|\n",
      "|1988|  292|\n",
      "|2014|  214|\n",
      "|1908|   18|\n",
      "|1986|  192|\n",
      "|1949|  351|\n",
      "|1930|  361|\n",
      "|1969|  137|\n",
      "|1964|  151|\n",
      "|1967|  127|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcxgQPR4glxw",
    "outputId": "737a84fc-da0b-46b7-9c8d-d939e29b5be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|1919|  634|\n",
      "+----+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(col(\"count\").desc()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpuHfGvWgl0w",
    "outputId": "f81543f9-bc7b-4f91-ddf0-3ae37c8f50c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'max(count)'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.max(df1['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDJr3c-y0rk"
   },
   "source": [
    "## 4. Show the list of movies such that for each film, the number of actors/actresses is at least five, and the number of genres it belongs to is at most two genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzXkIHHnhhd-",
    "outputId": "78fe5baa-554b-48c0-eb5b-7e0252e43b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+----+\n",
      "|                cast|          genres|               title|year|\n",
      "+--------------------+----------------+--------------------+----+\n",
      "|[Earle Foxe, Alie...|         [Drama]|  A Desperate Chance|1913|\n",
      "|[Charlotte Burton...|         [Drama]|    The Archeologist|1914|\n",
      "|[Charlotte Burton...|         [Drama]|At the Potter's W...|1914|\n",
      "|[Herbert Tracey, ...|        [Comedy]|    Back to the Farm|1914|\n",
      "|[Charlotte Burton...|              []|    The Beggar Child|1914|\n",
      "|[William Garwood,...|              []|       Billy's Rival|1914|\n",
      "|[B. Reeves Eason,...|         [Drama]| Break, Break, Break|1914|\n",
      "|[Charlotte Burton...|              []|       The Butterfly|1914|\n",
      "|[Charlotte Burton...|       [Western]|Calamity Anne's L...|1914|\n",
      "|[Charlie Chaplin,...|        [Comedy]|    The Star Boarder|1914|\n",
      "|[Sydney Ayres, Ja...|              []|A Story of Little...|1914|\n",
      "|[Sydney Ayres, Pe...|              []|The Story of the ...|1914|\n",
      "|[Charlotte Burton...|              []|    This Is th' Life|1914|\n",
      "|[Lon Chaney, Leat...|  [Crime, Drama]|   The Ace of Hearts|1921|\n",
      "|[Madge Kennedy, M...| [Comedy, Drama]|  The Purple Highway|1923|\n",
      "|[Douglas Fairbank...|              []| The Thief of Bagdad|1924|\n",
      "|[Kru, Chantui, Na...|   [Documentary]|Chang: A Drama of...|1927|\n",
      "|[H. B. Warner, An...|         [Drama]|     Sorrell and Son|1927|\n",
      "|[Sam De Grasse, V...|     [Adventure]|The Wreck of the ...|1927|\n",
      "|[Fredric March, O...|[Drama, Romance]|     Anthony Adverse|1936|\n",
      "+--------------------+----------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((f.size(df.cast)>=5)&(f.size(df.genres)<=2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL5CQj1hy6_j"
   },
   "source": [
    "## 5. Show the **movies** whose names are longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1sMv96LE2UZ",
    "outputId": "f47d971f-c37d-425f-b466-9887d4f3de4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+----+\n",
      "|cast|genres|               title|year|\n",
      "+----+------+--------------------+----+\n",
      "|  []|    []|Cornell-Columbia-...|1901|\n",
      "+----+------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(f.length(col('title')).desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr9Pw7rdzGAo"
   },
   "source": [
    "## 6. Show the movies whose name contains the word “fighting” (case-insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dcdjzq0tWWc3",
    "outputId": "25e917c6-c393-44ad-aaae-6ba10f36e39f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+----+\n",
      "|                cast|         genres|               title|year|\n",
      "+--------------------+---------------+--------------------+----+\n",
      "|[Bessie Love, Ann...|[Comedy, Drama]|  A Fighting Colleen|1919|\n",
      "|[Blanche Sweet, R...|      [Western]|     Fighting Cressy|1919|\n",
      "|[Harry T. Morey, ...|        [Drama]|    Fighting Destiny|1919|\n",
      "|[Tom Mix, Teddy S...|      [Western]|   Fighting for Gold|1919|\n",
      "|[Jack Perrin, Hoo...|      [Western]|  The Fighting Heart|1919|\n",
      "|[Art Acord, Mildr...|      [Western]|   The Fighting Line|1919|\n",
      "|[William Duncan, ...|       [Action]|  The Fighting Guide|1922|\n",
      "|[Tom Mix, Patsy R...|      [Western]| The Fighting Streak|1922|\n",
      "|[Richard Barthelm...|   [Historical]|  The Fighting Blade|1923|\n",
      "|[Ernest Torrence,...|       [Comedy]| The Fighting Coward|1924|\n",
      "|[Jack Hoxie, Hele...|      [Western]|       Fighting Fury|1924|\n",
      "|[Pat O'Malley, Ma...|        [Drama]|The Fighting Adve...|1924|\n",
      "|[Fred Thomson, Ha...|      [Western]|    The Fighting Sap|1924|\n",
      "|[Richard Talmadge...|       [Action]|  The Fighting Demon|1925|\n",
      "|[Billy Sullivan, ...|       [Sports]|       Fighting Fate|1925|\n",
      "|[George O'Brien, ...|        [Drama]|  The Fighting Heart|1925|\n",
      "|[Bob Reeves, Lew ...|      [Western]|       Fighting Luck|1925|\n",
      "|[Bill Cody, Jean ...|      [Western]|  The Fighting Smile|1925|\n",
      "|[William Haines, ...|        [Drama]| Fighting the Flames|1925|\n",
      "|[William Fairbank...|       [Action]|      Fighting Youth|1925|\n",
      "+--------------------+---------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(f.lower(col('title')).contains('fighting')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHjfLzpNzWLG"
   },
   "source": [
    "## 7. Show the list of distinct genres appearing in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RX0oTwVHbcY",
    "outputId": "04277ab6-b667-42f1-e384-7aa633263ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|          col|\n",
      "+-------------+\n",
      "|        Crime|\n",
      "|      Romance|\n",
      "|     Thriller|\n",
      "|      Slasher|\n",
      "|Found Footage|\n",
      "|    Adventure|\n",
      "|         Teen|\n",
      "| Martial Arts|\n",
      "|       Sports|\n",
      "|        Drama|\n",
      "|          War|\n",
      "|  Documentary|\n",
      "|       Family|\n",
      "|      Fantasy|\n",
      "|       Silent|\n",
      "|     Disaster|\n",
      "|        Legal|\n",
      "|      Mystery|\n",
      "| Supernatural|\n",
      "|     Suspense|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.explode(col('genres'))).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlCv5PmNzZaN"
   },
   "source": [
    "## 8. List all movies in which the actor Harrison Ford has participated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeNLgqK_zZmf",
    "outputId": "7a6bc7cf-a5d1-46e9-9110-d1eea6415023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+----+\n",
      "|                cast|           genres|               title|year|\n",
      "+--------------------+-----------------+--------------------+----+\n",
      "|[Constance Talmad...|[Romance, Comedy]|Experimental Marr...|1919|\n",
      "|[Constance Talmad...|         [Comedy]| Happiness a la Mode|1919|\n",
      "|[Constance Talmad...|         [Comedy]|Romance and Arabella|1919|\n",
      "|[Vivian Martin, H...|         [Comedy]|      The Third Kiss|1919|\n",
      "|[Harrison Ford, C...|         [Comedy]|The Veiled Adventure|1919|\n",
      "|[Constance Talmad...|         [Comedy]|          Who Cares?|1919|\n",
      "|[Vivian Martin, H...|          [Drama]|You Never Saw Suc...|1919|\n",
      "|[Norma Talmadge, ...|          [Drama]| The Wonderful Thing|1921|\n",
      "|[Alma Rubens, Har...|        [Mystery]|      Find the Woman|1922|\n",
      "|[Constance Talmad...|          [Drama]| The Primitive Lover|1922|\n",
      "|[Norma Talmadge, ...| [Romance, Drama]|     Smilin' Through|1922|\n",
      "|[Helen Jerome Edd...|          [Drama]|     When Love Comes|1922|\n",
      "|[Marion Davies, H...|     [Historical]| Little Old New York|1923|\n",
      "|[Madge Kennedy, H...|          [Drama]|     Three Miles Out|1924|\n",
      "|[Margaret Livings...|          [Drama]|           The Wheel|1925|\n",
      "|[Marie Prevost, H...|         [Comedy]|       Almost a Lady|1926|\n",
      "|[Harrison Ford, M...|          [Drama]| Hell's Four Hundred|1926|\n",
      "|[Harrison Ford, P...|         [Comedy]|   The Nervous Wreck|1926|\n",
      "|[Marie Prevost, H...|         [Comedy]|  Up in Mabel's Room|1926|\n",
      "|[Vera Reynolds, H...|         [Comedy]|         Golf Widows|1928|\n",
      "+--------------------+-----------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(f.array_contains(col('cast'), 'Harrison Ford')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlNi8cHWzZ9e"
   },
   "source": [
    "## 9. List all movies in which the actors/actresses whose names include the word “Lewis“ (case-insensitive) have participated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35otmzNOzaHs",
    "outputId": "d3bf6019-43d3-4b7e-cc67-d21ed9eac975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+----+\n",
      "|                cast|     genres|               title|year|\n",
      "+--------------------+-----------+--------------------+----+\n",
      "|[Charlotte Burton...|         []|       The Butterfly|1914|\n",
      "|[Pearl White, She...|    [Drama]|The Exploits of E...|1914|\n",
      "|[Charlotte Burton...|   [Comedy]| Mein Lieber Katrina|1914|\n",
      "|[Norma Talmadge, ...|    [Drama]|      Going Straight|1916|\n",
      "|[Dorothy Gish, Ra...|    [Drama]|Gretchen the Gree...|1916|\n",
      "|[Ben Lewis, Bessi...|  [Western]|     A Sister of Six|1916|\n",
      "|[Gail Kane, Lewis...|    [Drama]| The Bride's Silence|1917|\n",
      "|    [Mitchell Lewis]|    [Drama]|Nine-Tenths of th...|1918|\n",
      "|[Mitchell Lewis, ...|    [Drama]|The Faith of the ...|1919|\n",
      "|[Mary Pickford, R...|   [Comedy]|         The Hoodlum|1919|\n",
      "|[Mitchell Lewis, ...|    [Drama]|Jacques of the Si...|1919|\n",
      "|[Mitchell Lewis, ...|    [Drama]|The Last of His P...|1919|\n",
      "|[Lewis Stone, Jan...|    [Drama]|        Man's Desire|1919|\n",
      "|[Mary Miles Minte...|   [Comedy]|   Yvonne from Paris|1919|\n",
      "|[Mitchell Lewis, ...|    [Drama]|Nine-Tenths of th...|1919|\n",
      "|[Wedgwood Nowell,...|  [Mystery]|                 813|1920|\n",
      "|[Lewis Sargent, W...|[Adventure]|    Huckleberry Finn|1920|\n",
      "|[Pauline Frederic...|    [Drama]|             Salvage|1921|\n",
      "|[Viola Dana, Ralp...|   [Comedy]|The Five Dollar Baby|1922|\n",
      "|[Estelle Taylor, ...|    [Drama]|    A Fool There Was|1922|\n",
      "+--------------------+-----------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('cast_str', f.array_join(col('cast'), ' ')) \\\n",
    "  .filter(f.lower(col('cast_str')).contains('lewis')) \\\n",
    "  .drop('cast_str') \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "891rreb8zaUX"
   },
   "source": [
    "## 10. Show top five actors/actresses that have participated in most movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5b6rM2vzafM",
    "outputId": "5da8904f-b213-43ef-bd05-ee374b1dc201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|           actor|count|\n",
      "+----------------+-----+\n",
      "|    Harold Lloyd|  190|\n",
      "|     Hoot Gibson|  142|\n",
      "|      John Wayne|  136|\n",
      "|Charles Starrett|  116|\n",
      "|    Bebe Daniels|  103|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.explode(col('cast')).alias('actor')) \\\n",
    "  .groupBy('actor') \\\n",
    "  .count() \\\n",
    "  .orderBy(f.desc('count')) \\\n",
    "  .limit(5) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5WchdnCcyjZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWaVxJy4ctDE"
   },
   "source": [
    "# Exercise 2: RDD-based mainpulation\n",
    "\n",
    "\n",
    "*   The data is already in one ore more RDDs.\n",
    "*   You must not convert RDD to DF or use pure Python code.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdRrBxGt7Mxz"
   },
   "source": [
    "## 1. Consider a string s that includes only alphabetical letters and spaces. Check whether s is a palindrome (case-insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC7SPmqAP-8p"
   },
   "outputs": [],
   "source": [
    "def isPalin(s):\n",
    "    # remove non-alphabetical characters and lower\n",
    "    string = ''.join(char for char in s if char.isalnum()).lower()\n",
    "    words = spark.sparkContext.parallelize(string)\n",
    "    numRange = spark.sparkContext.parallelize(range(len(string)))\n",
    "    return numRange.zip(words).sortBy(lambda pair:pair[0] * -1).values().collect() == words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxpjVaQbCqJP",
    "outputId": "03d0635d-bca9-4ed8-bfd4-31fdd10f6d3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isPalin(\"Do geese see God?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APXulD8oI0Q2",
    "outputId": "3a4ed320-2d34-4a3e-9100-253b8979e336"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isPalin(\"Do you see God?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTx_x2ruc5Mi"
   },
   "source": [
    "## 2. Consider a string s that includes only alphabetical letters and spaces. Check whether s is a pangram (case-insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUFrR85R3kf7"
   },
   "outputs": [],
   "source": [
    "def isPangram(s):\n",
    "    # remove non-alphabetical characters and lower\n",
    "    string = ''.join(char for char in s if char.isalnum()).lower()\n",
    "    words = spark.sparkContext.parallelize(string)\n",
    "    return ''.join(words.groupBy(lambda c: c).sortBy(lambda c: c).keys().collect()) == 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohXUH7MVGRz0",
    "outputId": "184662f9-ccaa-47a4-874b-339002a15ddc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isPangram(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nC2ifuO0Ivor",
    "outputId": "37ec6ac7-1360-4d43-d98e-93ecf9354302"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isPangram(\"The quick brown fox jumps over the dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ca0DXd4ehYY"
   },
   "source": [
    "# Exercise 3: Frequent patterns and association rules mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laz3Iv8mfP3_"
   },
   "source": [
    "## 0. Load the data file: foodmart.csv\n",
    "\n",
    "\n",
    "*  A record is a tuple of binary values {0, 1}, each of which denotes the presence of an item (1: bought, 0: not bought).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjNqqxWflRcQ",
    "outputId": "dae7b4f1-9742-4ebd-be3c-5327f1059a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CSC14118' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nnthaofit/CSC14118.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-sfsk32nCRs"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"CSC14118/foodmart.csv\", header = True, inferSchema = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjV6v_qrfTmf"
   },
   "source": [
    "## 1. Convert the given data to the format required by Spark MLlib FPGrowth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pu_Daydzg6qE"
   },
   "outputs": [],
   "source": [
    "# Assuming the column names are correctly parsed from the header\n",
    "items = df.columns\n",
    "\n",
    "# Drop any rows with null values\n",
    "data = df.na.drop()\n",
    "\n",
    "# Convert the data to the format required by FP-Growth\n",
    "# Create an array column with the itemsets\n",
    "from pyspark.sql.functions import array, array_distinct\n",
    "data = data.select(array_distinct(array(*[col(item) for item in items])).alias(\"items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sz73Gq59S6LF",
    "outputId": "3a8f6c02-3981-4593-8c37-6306cf7672fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| items|\n",
      "+------+\n",
      "|[1, 0]|\n",
      "|[1, 0]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "|[0, 1]|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wtXuKiXg68I"
   },
   "source": [
    "## 2.\tApply Spark MLlib FPGrowth to the formatted data. Mine the set of frequent patterns with the minimum support of 0.1. Mine the set of association rules with the minimum confidence of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4Rou-9khM8y"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.1, minConfidence=0.9)\n",
    "model = fpGrowth.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uco8MhUYhNAe",
    "outputId": "258e95a3-ee65-49dd-de11-5f04395f5a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| items|freq|\n",
      "+------+----+\n",
      "|   [1]|2107|\n",
      "|[1, 0]|2107|\n",
      "|   [0]|2127|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freqItemsets = model.freqItemsets\n",
    "freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcMuuaxXS0PE",
    "outputId": "d24402d6-0074-4137-89ec-b6441a7dfc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+----+------------------+\n",
      "|antecedent|consequent|        confidence|lift|           support|\n",
      "+----------+----------+------------------+----+------------------+\n",
      "|       [0]|       [1]|0.9905970850963799| 1.0|0.9905970850963799|\n",
      "|       [1]|       [0]|               1.0| 1.0|0.9905970850963799|\n",
      "+----------+----------+------------------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "associationRules = model.associationRules\n",
    "associationRules.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCNBYGudnY0j"
   },
   "source": [
    "# Exercise 4: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBcYYGSXoH61"
   },
   "source": [
    "##0. Load the data file: mushroom.csv\n",
    "*   The data represents a collection of mushroom species. \n",
    "*   There are 8124 examples, each of which has 22 attributes and it is categorized into either “edible” (e) or “poisonous” (p) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COKEJ_iVnl2Q",
    "outputId": "b88ec31d-248c-46f2-8e07-58e94f06cb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CSC14118' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nnthaofit/CSC14118.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnF5ML6qo3Rz"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"CSC14118/mushrooms.csv\", header = True, inferSchema = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z3HcOQ5k_Oh"
   },
   "source": [
    "## 1.\tPrepare the train and test sets following the ratio 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrM5Pi4Rruma"
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQrHmRM2fCrK",
    "outputId": "920fd457-d539-488c-f969-52ba6afaf759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- cap-shape: string (nullable = true)\n",
      " |-- cap-surface: string (nullable = true)\n",
      " |-- cap-color: string (nullable = true)\n",
      " |-- bruises: string (nullable = true)\n",
      " |-- odor: string (nullable = true)\n",
      " |-- gill-attachment: string (nullable = true)\n",
      " |-- gill-spacing: string (nullable = true)\n",
      " |-- gill-size: string (nullable = true)\n",
      " |-- gill-color: string (nullable = true)\n",
      " |-- stalk-shape: string (nullable = true)\n",
      " |-- stalk-root: string (nullable = true)\n",
      " |-- stalk-surface-above-ring: string (nullable = true)\n",
      " |-- stalk-surface-below-ring: string (nullable = true)\n",
      " |-- stalk-color-above-ring: string (nullable = true)\n",
      " |-- stalk-color-below-ring: string (nullable = true)\n",
      " |-- veil-type: string (nullable = true)\n",
      " |-- veil-color: string (nullable = true)\n",
      " |-- ring-number: string (nullable = true)\n",
      " |-- ring-type: string (nullable = true)\n",
      " |-- spore-print-color: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- habitat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRV_XLPzmVph"
   },
   "source": [
    "## 2. Fit a decision tree model on the training set, using Spark MLlib DecisionTreeClassifier with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d6VTk_EmbUw"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Perform string indexing on the categorical features\n",
    "dt_indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"keep\")\n",
    "    for column in train.columns\n",
    "]\n",
    "\n",
    "# Combine the indexed features into a single vector column\n",
    "dt_assembler = VectorAssembler(inputCols=[column + \"_index\" for column in train.columns if column != \"class\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "# Perform string indexing on the target variable\n",
    "dt_class_indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "# Create a DecisionTreeClassifier instance\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Create a pipeline to chain the stages together\n",
    "dt_pipeline = Pipeline(stages=dt_indexers + [dt_class_indexer, dt_assembler, dt])\n",
    "\n",
    "# Fit the model on the training data\n",
    "dt_model = dt_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6xvCLsCgQ5v",
    "outputId": "66c47043-8dc4-4377-95eb-4fd53b379b56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|class|prediction|\n",
      "+-----+----------+\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    e|       0.0|\n",
      "|    p|       1.0|\n",
      "|    p|       1.0|\n",
      "|    p|       1.0|\n",
      "|    p|       1.0|\n",
      "|    p|       1.0|\n",
      "|    p|       1.0|\n",
      "|    p|       1.0|\n",
      "|    p|       1.0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the trained model on the test data to make predictions\n",
    "dt_predictions = dt_model.transform(test)\n",
    "\n",
    "# Show the predictions\n",
    "dt_predictions.select(\"class\", \"prediction\").sample(False, 0.01).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsM9K6iembd6"
   },
   "source": [
    "## 3. Fit a random forest model on the training set, using Spark MLlib RandomForestClassification with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gG0BAF_mfsC"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Perform string indexing on the categorical features\n",
    "rf_indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"keep\")\n",
    "    for column in train.columns\n",
    "]\n",
    "\n",
    "# Combine the indexed features into a single vector column\n",
    "rf_assembler = VectorAssembler(inputCols=[column + \"_index\" for column in train.columns if column != \"class\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "# Perform string indexing on the target variable\n",
    "rf_class_indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Create a pipeline to chain the stages together\n",
    "rf_pipeline = Pipeline(stages=rf_indexers + [rf_class_indexer, rf_assembler, rf])\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model = rf_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdbR2WBLi2qH",
    "outputId": "2a08d8ab-a8a3-47c6-c347-15914c96853e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|class|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|e    |0.0       |[0.973209022574466,0.02679097742553402]   |\n",
      "|e    |0.0       |[0.9796275803771992,0.020372419622800668] |\n",
      "|e    |0.0       |[0.9701012877678362,0.029898712232163848] |\n",
      "|e    |0.0       |[0.984140963712622,0.015859036287377986]  |\n",
      "|e    |0.0       |[0.984140963712622,0.015859036287377986]  |\n",
      "|e    |0.0       |[0.9722664545304559,0.02773354546954414]  |\n",
      "|e    |0.0       |[0.9620099814350898,0.037990018564910216] |\n",
      "|e    |0.0       |[0.971536274044453,0.028463725955547033]  |\n",
      "|p    |1.0       |[0.0010710879772972387,0.9989289120227027]|\n",
      "|p    |1.0       |[0.0010710879772972387,0.9989289120227027]|\n",
      "|p    |1.0       |[0.0010710879772972387,0.9989289120227027]|\n",
      "|p    |1.0       |[0.0043223195043908345,0.9956776804956092]|\n",
      "|p    |1.0       |[0.0010710879772972387,0.9989289120227027]|\n",
      "|p    |1.0       |[0.0010710879772972387,0.9989289120227027]|\n",
      "|p    |1.0       |[0.0043223195043908345,0.9956776804956092]|\n",
      "|p    |1.0       |[0.1432174641041562,0.8567825358958437]   |\n",
      "+-----+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "rf_predictions = rf_model.transform(test)\n",
    "\n",
    "# Show the predicted class labels and corresponding probabilities\n",
    "rf_predictions.select(\"class\", \"prediction\", \"probability\").sample(False, 0.01).limit(20).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDzdizZJmgD4"
   },
   "source": [
    "## 4. Evaluate the two models on the same test set using the following metrics: areaUnderROC and areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoCVUiwWmiL_",
    "outputId": "b2d0cd0a-b426-4b45-f01c-73f63ced430a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "Area under ROC: 0.998151690391459\n",
      "Area under PR: 0.9990446811771919\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate the decision tree model\n",
    "dt_evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "dt_roc = dt_evaluator.evaluate(dt_predictions, {dt_evaluator.metricName: \"areaUnderROC\"})\n",
    "dt_pr = dt_evaluator.evaluate(dt_predictions, {dt_evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Decision Tree Model\")\n",
    "print(f\"Area under ROC: {dt_roc}\")\n",
    "print(f\"Area under PR: {dt_pr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGX5hk0CqhzD",
    "outputId": "1a32793c-273a-4696-d472-6d731ad4346c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model\n",
      "Area under ROC: 1.0\n",
      "Area under PR: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the random forest model\n",
    "rf_evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "rf_roc = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"areaUnderROC\"})\n",
    "rf_pr = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Random Forest Model\")\n",
    "print(f\"Area under ROC: {rf_roc}\")\n",
    "print(f\"Area under PR: {rf_pr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJesU8qumiwu"
   },
   "source": [
    "##5. Chain the above steps into a single pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9PIlUkzqnU-",
    "outputId": "dccd0dc1-a113-4427-cbb6-73488483be96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "Area under ROC: 0.998151690391459\n",
      "Area under PR: 0.9990446811771919\n",
      "\n",
      "Random Forest Model\n",
      "Area under ROC: 1.0\n",
      "Area under PR: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the feature columns\n",
    "feature_columns = train.columns[1:]\n",
    "\n",
    "# Perform string indexing on the class column\n",
    "class_indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "# Perform string indexing on the feature columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column + \"_index\", handleInvalid=\"keep\")\n",
    "    for column in feature_columns\n",
    "]\n",
    "\n",
    "# Combine the indexed features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[column + \"_index\" for column in feature_columns],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "\n",
    "# Create a decision tree classifier and rename to avoid conflict\n",
    "dt_classifier = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\") \\\n",
    "                .setRawPredictionCol(\"dt_rawPrediction\") \\\n",
    "                .setProbabilityCol(\"dt_probability\") \\\n",
    "                .setPredictionCol(\"dt_prediction\")\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\") \\\n",
    "                .setRawPredictionCol(\"rf_rawPrediction\") \\\n",
    "                .setProbabilityCol(\"rf_probability\") \\\n",
    "                .setPredictionCol(\"rf_prediction\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=indexers + [class_indexer, assembler, dt_classifier, rf_classifier])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "evaluator.setRawPredictionCol(\"dt_rawPrediction\")\n",
    "roc_dt = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_dt = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "evaluator.setRawPredictionCol(\"rf_rawPrediction\")\n",
    "roc_rf = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "pr_rf = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Decision Tree Model\")\n",
    "print(f\"Area under ROC: {roc_dt}\")\n",
    "print(f\"Area under PR: {pr_dt}\")\n",
    "print()\n",
    "print(\"Random Forest Model\")\n",
    "print(f\"Area under ROC: {roc_rf}\")\n",
    "print(f\"Area under PR: {pr_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bV0K58Gqs7c"
   },
   "source": [
    "# Exercise 5: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CODuRchsbwl"
   },
   "source": [
    "##0. Load the data file: iris.csv\n",
    "\n",
    "*   The data represents a collection of iris plant species.\n",
    "*   There are 150 examples, each of which has 4 attributes and it is categorized into one of the three classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mm4cg-1HqtKl",
    "outputId": "9ad2afb6-0b32-4752-e947-27accfe9c47c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CSC14118' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nnthaofit/CSC14118.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO3rndauw_UG"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"CSC14118/iris.csv\", header = True, inferSchema = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufgbzoXED1op",
    "outputId": "8955759d-9896-4a40-f94f-29c5e61762cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: double (nullable = true)\n",
      " |-- SepalWidthCm: double (nullable = true)\n",
      " |-- PetalLengthCm: double (nullable = true)\n",
      " |-- PetalWidthCm: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi65_pscs4Za"
   },
   "source": [
    "## 1.\tCluster the data by using Spark MLlib KMeans with k = 2, 3, and 5, using Euclidean distance and cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yERy2dQhs3ny",
    "outputId": "4cf5a7bc-eaa7-4169-c862-609ccce4f89a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {'euclidean': KMeansModel: uid=KMeans_e8215a770200, k=2, distanceMeasure=euclidean, numFeatures=4,\n",
       "  'cosine': KMeansModel: uid=KMeans_ea3f4d7a452d, k=2, distanceMeasure=cosine, numFeatures=4},\n",
       " 3: {'euclidean': KMeansModel: uid=KMeans_3c8f79e0b7d1, k=3, distanceMeasure=euclidean, numFeatures=4,\n",
       "  'cosine': KMeansModel: uid=KMeans_ef9a89279e3a, k=3, distanceMeasure=cosine, numFeatures=4},\n",
       " 5: {'euclidean': KMeansModel: uid=KMeans_6997dda3ca35, k=5, distanceMeasure=euclidean, numFeatures=4,\n",
       "  'cosine': KMeansModel: uid=KMeans_5292d42ffd83, k=5, distanceMeasure=cosine, numFeatures=4}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Select the relevant columns for clustering\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "\n",
    "# Create a vector assembler to combine the feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "\n",
    "# Transform the DataFrame to include the feature vector column\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Define the values of k and distance metrics\n",
    "k_values = [2, 3, 5]\n",
    "distance_metrics = ['euclidean', 'cosine']\n",
    "\n",
    "# Create an empty dictionary to store the models\n",
    "models = {}\n",
    "\n",
    "# Iterate over the k and distance metric combinations\n",
    "for k in k_values:\n",
    "    # Create a nested dictionary for each k value\n",
    "    models[k] = {}\n",
    "    \n",
    "    for distance_metric in distance_metrics:\n",
    "        # Create a KMeans instance\n",
    "        kmeans = KMeans(k=k, distanceMeasure=distance_metric)\n",
    "        \n",
    "        # Fit the KMeans model on the data\n",
    "        model = kmeans.fit(data)\n",
    "        \n",
    "        # Add the model to the nested dictionary\n",
    "        models[k][distance_metric] = model\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvEt8s3ftlpp"
   },
   "source": [
    "## 2. Evaluate each of the above clustering results using silhoutte score. Which configuration yeilds the best clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztUBlUo4vNnk",
    "outputId": "ba5de605-bf64-408d-cb81-b84a3d7f3284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: k=2, distance=euclidean\n",
      "Silhouette Score: 0.8501515983265806\n",
      "------------------------------\n",
      "Model: k=2, distance=cosine\n",
      "Silhouette Score: 0.8462156076702896\n",
      "------------------------------\n",
      "Model: k=3, distance=euclidean\n",
      "Silhouette Score: 0.6537609085337411\n",
      "------------------------------\n",
      "Model: k=3, distance=cosine\n",
      "Silhouette Score: 0.6295595234430768\n",
      "------------------------------\n",
      "Model: k=5, distance=euclidean\n",
      "Silhouette Score: 0.631233808045035\n",
      "------------------------------\n",
      "Model: k=5, distance=cosine\n",
      "Silhouette Score: 0.4732362213519796\n",
      "------------------------------\n",
      "Best Configuration:\n",
      "k=2, Distance Metric: euclidean\n",
      "Best Silhouette Score: 0.8501515983265806\n"
     ]
    }
   ],
   "source": [
    "# Track the best configuration and its silhouette score\n",
    "best_configuration = None\n",
    "best_silhouette_score = float('-inf')\n",
    "\n",
    "# Evaluate the clustering results\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "# Iterate over the models and evaluate each one\n",
    "for k, distance_metrics_dict in models.items():\n",
    "    for distance_metric, model in distance_metrics_dict.items():\n",
    "        # Make predictions on the data\n",
    "        predictions = model.transform(data)\n",
    "        \n",
    "        # Evaluate the clustering results\n",
    "        silhouette_score = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Print the evaluation results\n",
    "        print(f\"Model: k={k}, distance={distance_metric}\")\n",
    "        print(f\"Silhouette Score: {silhouette_score}\")\n",
    "        print(\"------------------------------\")\n",
    "\n",
    "        # Check if this configuration has a better silhouette score\n",
    "        if silhouette_score > best_silhouette_score:\n",
    "            best_configuration = (k, distance_metric)\n",
    "            best_silhouette_score = silhouette_score\n",
    "\n",
    "# Print the best configuration and its silhouette score\n",
    "print(\"Best Configuration:\")\n",
    "print(f\"k={best_configuration[0]}, Distance Metric: {best_configuration[1]}\")\n",
    "print(f\"Best Silhouette Score: {best_silhouette_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD1REuMMzCv1"
   },
   "source": [
    "##3. Chain the above steps into a single pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFcNOyXczG3W",
    "outputId": "200ffba6-54f0-4cfd-d50c-70f4900e446d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k=2 and distance metric=euclidean, the Silhouette score is 0.8501515983265806\n",
      "For k=2 and distance metric=cosine, the Silhouette score is 0.8462156076702896\n",
      "For k=3 and distance metric=euclidean, the Silhouette score is 0.6537609085337411\n",
      "For k=3 and distance metric=cosine, the Silhouette score is 0.6295595234430768\n",
      "For k=5 and distance metric=euclidean, the Silhouette score is 0.631233808045035\n",
      "For k=5 and distance metric=cosine, the Silhouette score is 0.4732362213519796\n",
      "The best clustering configuration is k=2 and distance metric=euclidean with a Silhouette score of 0.8501515983265806\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Select the relevant columns for clustering\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "\n",
    "# Create a vector assembler to combine the feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "\n",
    "# Transform the DataFrame to include the feature vector column\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Define the values of k and distance metrics\n",
    "k_values = [2, 3, 5]\n",
    "distance_metrics = ['euclidean', 'cosine']\n",
    "\n",
    "# Create a list to store the pipeline stages\n",
    "pipeline_stages = []\n",
    "\n",
    "# Iterate over the k and distance metric combinations\n",
    "for k in k_values:\n",
    "    for distance_metric in distance_metrics:\n",
    "        # Create a KMeans instance\n",
    "        kmeans = KMeans(k=k, distanceMeasure=distance_metric)\n",
    "                \n",
    "        # Create a column name for the prediction\n",
    "        prediction_col = f\"prediction_k{k}_{distance_metric}\"\n",
    "        \n",
    "        # Set the prediction column name for the KMeans model\n",
    "        kmeans = kmeans.setPredictionCol(prediction_col)\n",
    "\n",
    "        # Add the KMeans model to the pipeline stages\n",
    "        pipeline_stages.append(kmeans)\n",
    "\n",
    "# Create a pipeline with the assembler and KMeans models\n",
    "pipeline = Pipeline(stages=[assembler] + pipeline_stages)\n",
    "\n",
    "# Fit the pipeline on the data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "# Transform the data using the fitted pipeline\n",
    "transformed_data = pipeline_model.transform(df)\n",
    "\n",
    "# Evaluate the clustering results and calculate Silhouette scores\n",
    "silhouette_scores = {}\n",
    "\n",
    "for k in k_values:\n",
    "    silhouette_scores[k] = {}\n",
    "    \n",
    "    for distance_metric in distance_metrics:\n",
    "        # Get the prediction column name for the current configuration\n",
    "        prediction_col = f\"prediction_k{k}_{distance_metric}\"\n",
    "        \n",
    "        # Evaluate the clustering using Silhouette score\n",
    "        evaluator = ClusteringEvaluator(predictionCol=prediction_col, featuresCol='features', metricName='silhouette')\n",
    "        score = evaluator.evaluate(transformed_data)\n",
    "        \n",
    "        # Store the Silhouette score in the dictionary\n",
    "        silhouette_scores[k][distance_metric] = score\n",
    "\n",
    "# Initialize variables to track the best configuration and score\n",
    "best_configuration = None\n",
    "best_score = -1.0\n",
    "\n",
    "# Iterate over the Silhouette scores and find the best configuration\n",
    "for k in k_values:\n",
    "    for distance_metric in distance_metrics:\n",
    "        score = silhouette_scores[k][distance_metric]\n",
    "        print(f\"For k={k} and distance metric={distance_metric}, the Silhouette score is {score}\")\n",
    "        \n",
    "        # Check if the current score is higher than the previous best score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_configuration = (k, distance_metric)\n",
    "\n",
    "# Print the best configuration and score\n",
    "print(f\"The best clustering configuration is k={best_configuration[0]} and distance metric={best_configuration[1]} with a Silhouette score of {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nbr9LCmqnqs"
   },
   "source": [
    "## 4. For each clustering result obtained above, count the number of examples that belong to each of the three species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRrlfAfDtTxw",
    "outputId": "fc7dd4d7-b233-49db-bbe1-71fedd54064f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for k=2 and distance metric=euclidean:\n",
      "+-----------------------+---------------+-----+\n",
      "|prediction_k2_euclidean|        Species|count|\n",
      "+-----------------------+---------------+-----+\n",
      "|                      1| Iris-virginica|   50|\n",
      "|                      0|    Iris-setosa|   50|\n",
      "|                      1|Iris-versicolor|   47|\n",
      "|                      0|Iris-versicolor|    3|\n",
      "+-----------------------+---------------+-----+\n",
      "\n",
      "\n",
      "Counts for k=2 and distance metric=cosine:\n",
      "+--------------------+---------------+-----+\n",
      "|prediction_k2_cosine|        Species|count|\n",
      "+--------------------+---------------+-----+\n",
      "|                   0| Iris-virginica|   50|\n",
      "|                   0|Iris-versicolor|   50|\n",
      "|                   1|    Iris-setosa|   50|\n",
      "+--------------------+---------------+-----+\n",
      "\n",
      "\n",
      "Counts for k=3 and distance metric=euclidean:\n",
      "+-----------------------+---------------+-----+\n",
      "|prediction_k3_euclidean|        Species|count|\n",
      "+-----------------------+---------------+-----+\n",
      "|                      2|Iris-versicolor|    4|\n",
      "|                      1| Iris-virginica|   50|\n",
      "|                      0|    Iris-setosa|   30|\n",
      "|                      1|Iris-versicolor|   46|\n",
      "|                      2|    Iris-setosa|   20|\n",
      "+-----------------------+---------------+-----+\n",
      "\n",
      "\n",
      "Counts for k=3 and distance metric=cosine:\n",
      "+--------------------+---------------+-----+\n",
      "|prediction_k3_cosine|        Species|count|\n",
      "+--------------------+---------------+-----+\n",
      "|                   2|Iris-versicolor|    5|\n",
      "|                   0|Iris-versicolor|   45|\n",
      "|                   1|    Iris-setosa|   50|\n",
      "|                   2| Iris-virginica|   50|\n",
      "+--------------------+---------------+-----+\n",
      "\n",
      "\n",
      "Counts for k=5 and distance metric=euclidean:\n",
      "+-----------------------+---------------+-----+\n",
      "|prediction_k5_euclidean|        Species|count|\n",
      "+-----------------------+---------------+-----+\n",
      "|                      4|Iris-versicolor|   23|\n",
      "|                      4| Iris-virginica|   17|\n",
      "|                      0| Iris-virginica|    1|\n",
      "|                      0|Iris-versicolor|   23|\n",
      "|                      1|    Iris-setosa|   50|\n",
      "|                      3|Iris-versicolor|    4|\n",
      "|                      2| Iris-virginica|   32|\n",
      "+-----------------------+---------------+-----+\n",
      "\n",
      "\n",
      "Counts for k=5 and distance metric=cosine:\n",
      "+--------------------+---------------+-----+\n",
      "|prediction_k5_cosine|        Species|count|\n",
      "+--------------------+---------------+-----+\n",
      "|                   4|Iris-versicolor|    3|\n",
      "|                   4| Iris-virginica|   31|\n",
      "|                   2|Iris-versicolor|    4|\n",
      "|                   3| Iris-virginica|    8|\n",
      "|                   0|Iris-versicolor|   43|\n",
      "|                   1|    Iris-setosa|   50|\n",
      "|                   2| Iris-virginica|   11|\n",
      "+--------------------+---------------+-----+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the Silhouette scores and clustering results\n",
    "for k in k_values:\n",
    "    for distance_metric in distance_metrics:\n",
    "        # Get the prediction column name for the current configuration\n",
    "        prediction_col = f\"prediction_k{k}_{distance_metric}\"\n",
    "        \n",
    "        # Select the prediction column and the species column from the transformed data\n",
    "        selected_data = transformed_data.select(prediction_col, 'Species')\n",
    "        \n",
    "        # Count the number of examples for each species in the current clustering result\n",
    "        counts = selected_data.groupBy(prediction_col, 'Species').count()\n",
    "        \n",
    "        # Print the counts for the current clustering result\n",
    "        print(f\"Counts for k={k} and distance metric={distance_metric}:\")\n",
    "        counts.show()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7yOZSetvUBl"
   },
   "source": [
    "# Exercise 6: Network manipulation with Spark GraphFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrP0kQogwzbZ"
   },
   "source": [
    "##0. Load the data files: users.txt and followers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFGw_RhPw8Xt",
    "outputId": "d5149bfe-4fc6-4e83-ed8c-26b0fffd5e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CSC14118' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nnthaofit/CSC14118.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64SSjem5xE91",
    "outputId": "75a95fd4-bd27-436d-a253-673e76950596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------------+\n",
      "| id|     username|           name|\n",
      "+---+-------------+---------------+\n",
      "|  1|  BarackObama|   Barack Obama|\n",
      "|  2|     ladygaga|Goddess of Love|\n",
      "|  3|      jeresig|     John Resig|\n",
      "|  4| justinbieber|  Justin Bieber|\n",
      "|  6|matei_zaharia|  Matei Zaharia|\n",
      "|  7|      odersky| Martin Odersky|\n",
      "|  8|      anonsys|           null|\n",
      "+---+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the file into a DataFrame\n",
    "users_df = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"delimiter\", \",\").load(\"CSC14118/users.txt\")\n",
    "\n",
    "# Rename the columns\n",
    "users_df = users_df.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"username\").withColumnRenamed(\"_c2\", \"name\")\n",
    "\n",
    "# Show the DataFrame\n",
    "users_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJ3KphaoREzS",
    "outputId": "d23545c0-2e5b-4137-8a0b-a6592e47bab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  2|  1|\n",
      "|  4|  1|\n",
      "|  1|  2|\n",
      "|  6|  3|\n",
      "|  7|  3|\n",
      "|  7|  6|\n",
      "|  6|  7|\n",
      "|  3|  7|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the file into a DataFrame\n",
    "followers_df = spark.read.format(\"csv\").option(\"header\", \"false\").option(\"delimiter\", \" \").load(\"CSC14118/followers.txt\")\n",
    "\n",
    "# Rename the columns\n",
    "followers_df = followers_df.withColumnRenamed(\"_c0\", \"src\").withColumnRenamed(\"_c1\", \"dst\")\n",
    "\n",
    "# Show the DataFrame\n",
    "followers_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKwYijdmwCOa"
   },
   "source": [
    "##1.\tConstruct a graph from the given data to demonstrate a tiny social network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M52HaFYgU-jU",
    "outputId": "885d9ae8-6235-4796-c051-31cb14595dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.22.4)\n",
      "Collecting nose (from graphframes)\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nose, graphframes\n",
      "Successfully installed graphframes-0.6 nose-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNsROeTQvUVE",
    "outputId": "bf883b4d-7ce6-4fc0-b7ba-1ec38c7cedb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------------+\n",
      "| id|     username|           name|\n",
      "+---+-------------+---------------+\n",
      "|  1|  BarackObama|   Barack Obama|\n",
      "|  2|     ladygaga|Goddess of Love|\n",
      "|  3|      jeresig|     John Resig|\n",
      "|  4| justinbieber|  Justin Bieber|\n",
      "|  6|matei_zaharia|  Matei Zaharia|\n",
      "|  7|      odersky| Martin Odersky|\n",
      "|  8|      anonsys|           null|\n",
      "+---+-------------+---------------+\n",
      "\n",
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  2|  1|\n",
      "|  4|  1|\n",
      "|  1|  2|\n",
      "|  6|  3|\n",
      "|  7|  3|\n",
      "|  7|  6|\n",
      "|  6|  7|\n",
      "|  3|  7|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "# Create the graph\n",
    "graph = GraphFrame(users_df, followers_df)\n",
    "\n",
    "# Print the vertices and edges\n",
    "graph.vertices.show()\n",
    "graph.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEsvLBaHvUdK"
   },
   "source": [
    "##2.\tApply Graphs graphPageRank to the network to obtain a ranking list of users in terms of followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcDmK-3BvUjQ",
    "outputId": "ebb74c2a-1909-48d8-b601-f977acc1610b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------------+-------------------+\n",
      "| id|     username|           name|           pagerank|\n",
      "+---+-------------+---------------+-------------------+\n",
      "|  1|  BarackObama|   Barack Obama| 1.6799960991181582|\n",
      "|  2|     ladygaga|Goddess of Love|  1.597343003059732|\n",
      "|  7|      odersky| Martin Odersky| 1.4475660109319795|\n",
      "|  3|      jeresig|     John Resig| 1.1216981830374868|\n",
      "|  6|matei_zaharia|  Matei Zaharia|  0.794609271048275|\n",
      "|  8|      anonsys|           null|0.17939371640218368|\n",
      "|  4| justinbieber|  Justin Bieber|0.17939371640218368|\n",
      "+---+-------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply PageRank algorithm\n",
    "pagerank = graph.pageRank(resetProbability=0.15, tol=0.01)\n",
    "\n",
    "# Get the PageRank scores DataFrame\n",
    "pagerank_scores = pagerank.vertices\n",
    "\n",
    "# Sort the DataFrame by PageRank score in descending order\n",
    "ranking = pagerank_scores.orderBy(pagerank_scores.pagerank.desc())\n",
    "\n",
    "# Show the ranking list\n",
    "ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpijszupvU0Z"
   },
   "source": [
    "##3. Find connected components on the graph, using Graphs connectedComponents or stronglyConnectedComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XNfwRaNb1CK",
    "outputId": "fc5722d9-424c-4f2d-efdf-eeb8aa9fe232"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------------+-------------+\n",
      "| id|     username|           name|    component|\n",
      "+---+-------------+---------------+-------------+\n",
      "|  1|  BarackObama|   Barack Obama|1236950581248|\n",
      "|  2|     ladygaga|Goddess of Love|1236950581248|\n",
      "|  3|      jeresig|     John Resig|  25769803776|\n",
      "|  4| justinbieber|  Justin Bieber|1236950581248|\n",
      "|  6|matei_zaharia|  Matei Zaharia|  25769803776|\n",
      "|  7|      odersky| Martin Odersky|  25769803776|\n",
      "|  8|      anonsys|           null| 223338299392|\n",
      "+---+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the checkpoint directory\n",
    "spark.sparkContext.setCheckpointDir(\"/content/checkpoint_directory\")\n",
    "\n",
    "# Find the connected components\n",
    "connected_components = graph.connectedComponents()\n",
    "\n",
    "# Show the connected components\n",
    "connected_components.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pegb3QOvU5w",
    "outputId": "18301a51-a704-4185-b483-749ae4f46994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------------+-------------+\n",
      "| id|     username|           name|    component|\n",
      "+---+-------------+---------------+-------------+\n",
      "|  8|      anonsys|           null| 223338299392|\n",
      "|  4| justinbieber|  Justin Bieber|1425929142272|\n",
      "|  6|matei_zaharia|  Matei Zaharia|  25769803776|\n",
      "|  3|      jeresig|     John Resig|  25769803776|\n",
      "|  1|  BarackObama|   Barack Obama|1236950581248|\n",
      "|  7|      odersky| Martin Odersky|  25769803776|\n",
      "|  2|     ladygaga|Goddess of Love|1236950581248|\n",
      "+---+-------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the strongly connected components\n",
    "strongly_connected_components = graph.stronglyConnectedComponents(maxIter=10)\n",
    "\n",
    "# Show the strongly connected components\n",
    "strongly_connected_components.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_shG3rNyeJP6"
   },
   "source": [
    "### Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "Vpg4_KVweHv8",
    "outputId": "e3049433-33f7-4e86-d3c2-452fcf1a2d8b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the graph using the vertices and edges DataFrames\n",
    "graph_plt = nx.from_pandas_edgelist(followers_df.toPandas(), 'src', 'dst')\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(graph_plt, with_labels=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Petw2zs7yKrt",
    "tRKbh8RdyYZN",
    "o8egu2pgyg-y",
    "EHDJr3c-y0rk"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
